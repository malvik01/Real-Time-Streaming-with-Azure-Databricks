{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a39961-ef88-4c98-bdaf-32599b4e4106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-time Data Processing with Azure Databricks (and Event Hubs)\n",
    "\n",
    "This notebook demonstrates the below architecture to build real-time data pipelines.\n",
    "![Solution Architecture](https://raw.githubusercontent.com/malvik01/Real-Time-Streaming-with-Azure-Databricks/main/Azure%20Solution%20Architecture.png)\n",
    "\n",
    "\n",
    "- Data Sources: Streaming data from IoT devices or social media feeds. (Simulated in Event Hubs)\n",
    "- Ingestion: Azure Event Hubs for capturing real-time data.\n",
    "- Processing: Azure Databricks for stream processing using Structured Streaming.\n",
    "- Storage: Processed data stored Azure Data Lake (Delta Format).\n",
    "- Visualisation: Data visualized using Power BI.\n",
    "\n",
    "\n",
    "### Azure Services Required\n",
    "- Databricks Workspace (Unity Catalog enabled)\n",
    "- Azure Data Lake Storage (Premium)\n",
    "- Azure Event Hub (Basic Tier)\n",
    "\n",
    "### Azure Databricks Configuration Required\n",
    "- Single Node Compute Cluster: `12.2 LTS (includes Apache Spark 3.3.2, Scala 2.12)`\n",
    "- Maven Library installed on Compute Cluster: `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a287c0-1c5e-4649-ad56-a7b03707a663",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376948f-980e-4887-a319-45a525172d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5cc9f6-21cd-4201-a324-e97122c8203f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The code block below creates the catalog and schemas for our solution. \n",
    "\n",
    "The approach utilises a multi-hop data storage architecture (medallion), consisting of bronze, silver, and gold schemas within a 'streaming' catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check if catalog already exists\ncheck if bronze schema already exists\ncheck if silver schema already exists\ncheck if gold schema already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"create catalog streaming;\")\n",
    "except:\n",
    "    print('check if catalog already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.bronze;\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.silver\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.gold;\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61da0-3b55-4941-8436-2411646fa4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8806de51-7d1e-4dcc-8981-27f5a3447f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Set up Azure Event hubs connection string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "# Replace with your Event Hub namespace, name, and key\n",
    "connectionString = \"Paste connection string here\"\n",
    "eventHubName = \"Type event hub name here\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b28ff3-52b8-4a3f-ad67-758854096a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and writing the stream to the bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab57af3-36c7-4ffa-b13f-a833d7a2dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading stream: Load data from Azure Event Hub into DataFrame 'df' using the previously configured settings\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load() \\\n",
    "\n",
    "# Displaying stream: Show the incoming streaming data for visualization and debugging purposes\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Persist the streaming data to a Delta table 'streaming.bronze.weather' in 'append' mode with checkpointing\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/bronze/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.bronze.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c824846-2235-4e3b-8e75-68281293d50b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Defining the schema for the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b661190-f917-448b-9093-1017dde3bb25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the schema for the JSON object\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"temperature\", IntegerType()),\n",
    "    StructField(\"humidity\", IntegerType()),\n",
    "    StructField(\"windSpeed\", IntegerType()),\n",
    "    StructField(\"windDirection\", StringType()),\n",
    "    StructField(\"precipitation\", IntegerType()),\n",
    "    StructField(\"conditions\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd659c9-65fc-4112-be6e-06f63196311f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, transforming and writing the stream from the bronze to the silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading and Transforming: Load streaming data from the 'streaming.bronze.weather' Delta table, cast 'body' to string, parse JSON, and select specific fields\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.bronze.weather\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema))\\\n",
    "    .select(\"body.temperature\", \"body.humidity\", \"body.windSpeed\", \"body.windDirection\", \"body.precipitation\", \"body.conditions\", col(\"enqueuedTime\").alias('timestamp'))\n",
    "\n",
    "# Displaying stream: Visualize the transformed data in the DataFrame for verification and analysis\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Save the transformed data to the 'streaming.silver.weather' Delta table in 'append' mode with checkpointing for data reliability\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/silver/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.silver.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c07779-5a57-4f70-8c90-34554f8a82b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, aggregating and writing the stream from the silver to the gold layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating Stream: Read from 'streaming.silver.weather', apply watermarking and windowing, and calculate average weather metrics\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.silver.weather\")\\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(avg(\"temperature\").alias('temperature'), avg(\"humidity\").alias('humidity'), avg(\"windSpeed\").alias('windSpeed'), avg(\"precipitation\").alias('precipitation'))\\\n",
    "\t.select('window.start', 'window.end', 'temperature', 'humidity', 'windSpeed', 'precipitation')\n",
    "\n",
    "# Displaying Aggregated Stream: Visualize aggregated data for insights into weather trends\n",
    "df.display()\n",
    "\n",
    "# Writing Aggregated Stream: Store the aggregated data in 'streaming.gold.weather_aggregated' with checkpointing for data integrity\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/weather_summary\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.gold.weather_summary\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4016826182764860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time Data Processing with Azure Databricks (and Event Hubs)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
